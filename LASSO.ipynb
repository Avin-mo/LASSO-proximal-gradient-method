{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ℓ₁-Regularized Least Squares (LASSO) via Proximal Gradient (ISTA)\n",
    "\n",
    "This notebook presents a small project developed as part of my effort to learn about proximal methods and optimization algorithms.\n",
    "\n",
    "While studying different regression problems and numerical solvers, I became interested in the following question:\n",
    "\n",
    "**How does the structure of an objective function influence the optimization method used to solve it?**\n",
    "\n",
    "Although this is a simple and well-studied question, the specific answer is not the primary focus of this project.  \n",
    "Instead, the goal is to understand *where* these structural effects emerge within the algorithm and *how* they influence the algorithm’s behavior throughout the optimization process.\n",
    "\n",
    "Rather than treating optimization methods as black boxes, this notebook connects the mathematical properties of the objective function with the observed computational behavior of the algorithm in practice.\n",
    "\n",
    "The focus is ℓ₁-regularized linear regression, a problem whose non-smooth geometry fundamentally changes how optimization must be performed and provides a clear setting for studying these effects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why ℓ₁ Regularization?\n",
    "\n",
    "ℓ₁-regularized regression provides one of the simplest examples of an optimization problem where standard smooth methods are no longer directly applicable.\n",
    "\n",
    "For comparison, consider the two common regularized least-squares formulations.\n",
    "\n",
    "The ℓ₂-regularized (ridge) problem is:\n",
    "\n",
    "$$\n",
    "\\min_x \\; \\frac12\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_2^2\n",
    "$$\n",
    "\n",
    "This objective is fully smooth and can be efficiently minimized using classical methods such as **gradient descent**, **Newton’s method**, or **conjugate gradient methods**, all of which rely on differentiability of the objective.\n",
    "\n",
    "In contrast, the ℓ₁-regularized (LASSO) problem is:\n",
    "\n",
    "$$\n",
    "\\min_x \\; \\frac12\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n",
    "$$\n",
    "\n",
    "While the least-squares term remains smooth, the ℓ₁ regularization term introduces a non-smooth component. In particular, the ℓ₁ norm is **non-differentiable at zero**, which creates sharp kinks in the geometry of the objective.\n",
    "\n",
    "Because of this non-smoothness, standard gradient-based methods are no longer well-defined at points where coefficients approach zero, and Newton-type methods that rely on second-order smoothness also break down.\n",
    "\n",
    "I chose ℓ₁ regularization rather than ℓ₂ because it represents the simplest modification of a smooth regression problem that introduces a fundamental algorithmic complication. The presence of the non-differentiable ℓ₁ term changes not only the solution but also the class of optimization methods that can be applied.\n",
    "\n",
    "Thus, ℓ₁-regularized regression provides a clean and controlled setting for studying how the structure of an objective function, specifically non-smoothness and sparsity, forces changes in optimization algorithm design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation\n",
    "\n",
    "The problem studied in this project is ℓ₁-regularized least squares:\n",
    "\n",
    "$$\n",
    "\\min_x \\; \\frac12\\|Ax - b\\|_2^2 + \\lambda \\|x\\|_1\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $A \\in \\mathbb{R}^{m \\times n}$ is the design matrix \n",
    "- $b \\in \\mathbb{R}^m$ is the observation vector \n",
    "- $x \\in \\mathbb{R}^n$ is the parameter vector \n",
    "- $\\lambda > 0$ controls the tradeoff between data fidelity and sparsity\n",
    "\n",
    "The objective consists of two components:\n",
    "\n",
    "• a quadratic least-squares term that measures how well the model fits the data  \n",
    "• an ℓ₁ penalty that encourages sparsity in the coefficients  \n",
    "\n",
    "The regularization parameter $\\lambda$ determines how strongly sparsity is enforced relative to data fidelity.\n",
    "\n",
    "The least-squares term is smooth and has a Lipschitz-continuous gradient, so from an optimization perspective it behaves well. The ℓ₁ term, however, is fundamentally different: it penalizes all nonzero coefficients uniformly and is non-differentiable at zero.\n",
    "\n",
    "The factor of one-half in the quadratic term is included for convenience, as it simplifies the gradient without affecting the solution.\n",
    "\n",
    "Although the full objective is convex, so existence and uniqueness are not the primary concerns, the real challenge arises from combining a smooth term with a non-smooth one. This composite structure creates a geometry that standard gradient-based methods are not designed to handle.\n",
    "\n",
    "As a result, the central question becomes not simply what the objective is, but how to optimize it in a way that respects this structure, using gradient information where it applies while explicitly handling the non-smooth behavior introduced by the ℓ₁ penalty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Objective Structure to Algorithmic Behavior: ISTA\n",
    "\n",
    "The composite structure of the ℓ₁-regularized objective fundamentally changes how the problem must be optimized. Standard gradient descent is no longer appropriate because near zero the objective is not smooth, and gradient information alone does not provide a meaningful descent direction.\n",
    "\n",
    "This places the problem in the class of composite objectives, consisting of a smooth loss function combined with a non-smooth regularizer. For problems of this form, proximal gradient methods provide a natural optimization framework.\n",
    "\n",
    "Rather than smoothing or approximating the ℓ₁ term, proximal gradient methods handle it directly by pairing a gradient step on the smooth component with a proximal step for the non-smooth component.\n",
    "\n",
    "For ℓ₁-regularized least squares, this leads to the Iterative Soft-Thresholding Algorithm (ISTA), which performs the update:\n",
    "\n",
    "$$\n",
    "x^{k+1} = \\mathrm{soft}\\left(x^k - \\alpha A^\\top(Ax^k - b), \\alpha \\lambda\\right)\n",
    "$$\n",
    "\n",
    "where the step size is chosen as:\n",
    "\n",
    "$$\n",
    "\\alpha = \\frac{1}{L}, \\quad L = \\|A\\|_2^2\n",
    "$$\n",
    "\n",
    "For the ℓ₁ norm, the proximal operator has a closed-form solution given by soft-thresholding:\n",
    "\n",
    "$$\n",
    "\\mathrm{soft}(z,\\alpha\\lambda)_i\n",
    "= \\mathrm{sign}(z_i)\\max\\left(|z_i| - \\alpha\\lambda,\\, 0\\right)\n",
    "$$\n",
    "\n",
    "This single update directly reflects the composite structure of the objective.\n",
    "\n",
    "The gradient step reduces the data-fitting error, while the soft-thresholding step explicitly enforces sparsity by shrinking coefficients toward zero and setting small values exactly to zero.\n",
    "\n",
    "I chose to implement ISTA directly, rather than relying on existing solvers, in order to observe how the mechanics of the algorithm reflect the geometry introduced by the ℓ₁ penalty.\n",
    "\n",
    "What is most interesting is not simply the algorithm itself, but the behavior it produces. Sparsity is enforced at every iteration, and in practice the iterates tend to identify the active set early in the optimization process. After this point, convergence effectively occurs in a much lower-dimensional subspace.\n",
    "\n",
    "This update rule drives all of the phenomena observed in the results that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "To study the behavior of ISTA in a controlled setting, we construct a synthetic sparse regression problem with known ground truth.\n",
    "\n",
    "The design matrix $$A \\in \\mathbb{R}^{m \\times n}$$ is generated with independent Gaussian entries. A sparse true coefficient vector $x^*$ is then constructed by selecting $k$ random indices and assigning them nonzero values drawn from a Gaussian distribution, while all remaining entries are set to zero.\n",
    "\n",
    "Observations are generated according to the linear model:\n",
    "\n",
    "$$\n",
    "b = Ax^* + \\varepsilon\n",
    "$$\n",
    "\n",
    "where $\\varepsilon$ is small Gaussian noise with standard deviation specified by `noise_std`.\n",
    "\n",
    "This setup provides a simple yet diagnostic test problem. Because the true sparse solution is known, it allows direct examination of:\n",
    "\n",
    "• how sparsity emerges during optimization  \n",
    "• whether the correct support is identified  \n",
    "• how recovery accuracy depends on the regularization parameter  \n",
    "\n",
    "The goal is not realism, but clarity in observing the effects of ℓ₁ regularization and the behavior of the ISTA algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data(m=120, n=300, k=15, noise_std=0.01, seed=0):\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    A = rng.normal(size=(m, n))\n",
    "\n",
    "    x_true = np.zeros(n)\n",
    "    idx = rng.choice(n, size=k, replace=False)\n",
    "    x_true[idx] = rng.normal(loc=0.0, scale=1.0, size=k)\n",
    "\n",
    "    noise = noise_std * rng.normal(size=m)\n",
    "    b = A @ x_true + noise  # b = Ax* + noise\n",
    "\n",
    "    return A, b, x_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design walkthrough\n",
    "\n",
    "- `rng = np.random.default_rng(seed)`  \n",
    "  Uses NumPy’s modern random generator so results are reproducible via `seed`, the initial state of zero.\n",
    "\n",
    "- `A = rng.normal(size=(m, n))`  \n",
    "  Creates a Gaussian design matrix A. This is a standard synthetic setting because it avoids structure that could hide the effect of ℓ₁ regularization.\n",
    "\n",
    "- `x_true = np.zeros(n)`\n",
    "  Initializes a coefficient vector of length `n` so sparsity is explicit.\n",
    "\n",
    "- `idx = rng.choice(n, size=k, replace=False)`  \n",
    "  RSelects exactly `k` random active coordinates without replacement. This fixes the sparsity level and makes support recovery measurable.\n",
    "\n",
    "- `x_true[idx] = rng.normal(loc=0.0, scale=1.0, size=k)`  \n",
    "  Assigns nonzero coefficients with moderate magnitude. Gaussian values make the signal neither too uniform nor adversarial.\n",
    "\n",
    "- `noise = noise_std * rng.normal(size=m)`  \n",
    "  Adds small Gaussian noise, controlled by `noise_std`, to avoid a perfectly noiseless case.\n",
    "\n",
    "- `b = A @ x_true + noise`  \n",
    "  Generates observations from the linear model $b = Ax^* + \\varepsilon$.\n",
    "\n",
    "- `return A, b, x_true`  \n",
    "  Returns everything needed for optimization and evaluation: the data, the observations, and the ground truth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Core Operations\n",
    "\n",
    "We implement:\n",
    "\n",
    "- the LASSO objective  \n",
    "- gradient of the smooth term  \n",
    "- soft-thresholding (prox operator)  \n",
    "- Lipschitz step size  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso_objective(A, b, x, lam):\n",
    "    r = A @ x - b\n",
    "    return 0.5 * (r @ r) + lam * np.linalg.norm(x, 1)\n",
    "\n",
    "def grad_least_squares(A, b, x):\n",
    "    return A.T @ (A @ x - b)\n",
    "\n",
    "def soft_threshold(z, t):\n",
    "    return np.sign(z) * np.maximum(np.abs(z) - t, 0.0)\n",
    "\n",
    "def lipschitz_step(A):\n",
    "    smax = np.linalg.norm(A, 2)\n",
    "    L = smax * smax\n",
    "    return 1.0 / L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ISTA Implementation\n",
    "\n",
    "We now combine the gradient step and proximal step into the full ISTA iteration while tracking:\n",
    "\n",
    "- objective value  \n",
    "- number of nonzeros (sparsity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ista(A, b, lam, max_iter=500, tol=1e-6):\n",
    "\n",
    "    n = A.shape[1]\n",
    "    x = np.zeros(n)\n",
    "    step = lipschitz_step(A)\n",
    "\n",
    "    history = {\"obj\": [], \"nnz\": []}\n",
    "\n",
    "    for it in range(max_iter):\n",
    "\n",
    "        g = grad_least_squares(A, b, x)\n",
    "        x_next = soft_threshold(x - step * g, step * lam)\n",
    "\n",
    "        obj = lasso_objective(A, b, x_next, lam)\n",
    "        history[\"obj\"].append(obj)\n",
    "        history[\"nnz\"].append(int(np.count_nonzero(x_next)))\n",
    "\n",
    "        if np.linalg.norm(x_next - x) <= tol * max(1.0, np.linalg.norm(x)):\n",
    "            x = x_next\n",
    "            break\n",
    "\n",
    "        x = x_next\n",
    "\n",
    "    return x, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recovered nonzeros: 235\n",
      "Final objective: 1.1901585916932367\n",
      "L2 error: 2.7684570543592613\n"
     ]
    }
   ],
   "source": [
    "A, b, x_true = make_synthetic_data()\n",
    "\n",
    "lam = 0.05\n",
    "x_hat, hist = ista(A, b, lam, max_iter=1000)\n",
    "\n",
    "print(\"Recovered nonzeros:\", np.count_nonzero(x_hat))\n",
    "print(\"Final objective:\", hist[\"obj\"][-1])\n",
    "print(\"L2 error:\", np.linalg.norm(x_hat - x_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (2662093673.py, line 2)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mplt.figure()\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# ---- Plot 1: Objective value ----\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"obj\"])\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective value\")\n",
    "    plt.title(f\"ISTA convergence (λ = {lam}) - Objective vs. Iterations\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---- Plot 2: Nonzero count ----\n",
    "    plt.figure()\n",
    "    plt.plot(hist[\"nnz\"])\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Number of nonzeros\")\n",
    "    plt.title(f\"Sparsity over iterations (λ = {lam})\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # ---- Plot 3: Support comparison (x_true vs x_hat) ----\n",
    "    plt.figure()\n",
    "    plt.stem(x_true, linefmt='C0-', markerfmt='C0o', basefmt=\" \", label='x_true')\n",
    "    plt.stem(x_hat,  linefmt='C1-', markerfmt='C1x', basefmt=\" \", label='x_hat')\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(f\"True vs recovered coefficients (λ = {lam})\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # ---- Plot 4: lambda sweep ----\n",
    "    lams = [0.005, 0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "    final_nnz = []\n",
    "    final_err = []\n",
    "\n",
    "    for lam_i in lams:\n",
    "        x_hat_i, hist_i = ista(A, b, lam_i, max_iter=2000)\n",
    "        final_nnz.append(np.count_nonzero(x_hat_i))\n",
    "        final_err.append(np.linalg.norm(x_hat_i - x_true))\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lams, final_nnz, marker='o')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"λ\")\n",
    "    plt.ylabel(\"Final number of nonzeros\")\n",
    "    plt.title(\"Sparsity vs λ\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(lams, final_err, marker='o')\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"λ\")\n",
    "    plt.ylabel(\"||x_hat - x_true||_2\")\n",
    "    plt.title(\"Recovery error vs λ\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
